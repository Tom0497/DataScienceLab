{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA6202: Laboratorio de Ciencia de Datos\n",
    "\n",
    "**Profesor: Nicolás Caro**\n",
    "\n",
    "**27/04/2020 - C8 S4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamiento de datos y Exploración \n",
    "\n",
    "La el manejo de la información contenida en los datos es la razón principal de la construcción de modelos y esquemas de análisis sobre ellos. Tal información se ve afectada por la calidad de los datos, que en segunda instancia, determina el rendimiento de los modelos planteados. Esto hace que sea critico asegurar un preprocesado y una buena examinación de los datasets a trabajar.\n",
    "\n",
    "El contenido de esta cátedra, se centra en las técnicas esenciales para el preprocesado de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de datos exploratorio\n",
    "\n",
    "El análisis exploratorio de los datos (EDA en inglés) consiste e utilizar técnicas de sumarización o agregación, con el fin de conocer la distribución de los datos, confirmar hipótesis y contrastar información. Existen muchas maneras de explorar los datos, por ejemplo, se pueden generan visualizaciones, descripciones del conjunto de datos, se pueden también generar agrupaciones y obtener patrones de tales agrupaciones. \n",
    "\n",
    "Un concepto recurrente en el análisis exploratorio de datos consiste en el *perfilamiento* de datos. Este hace referencia a la sumarización por medio de estadística descriptiva, aquí existe una variedad de herramientas que pueden ayudar a comprender mejor los datos disponibles. La meta del perfilamiento de datos consiste en generar respuestas y conocimiento en torno al fenómeno que los datos reflejan. En función de los perfiles generados, se puede tener una idea de la calidad del dataset con lo cual es posible decidir como transformar las variables a disposición. A continuación se da una guía a seguir al momento de explorar los datos\n",
    "\n",
    "### Perfilamiento Univariado\n",
    "\n",
    "El punto inicial para comprender la naturaleza de una variable, pasa por caracterizar la forma de su distribución, un histograma permite obtener ideas sobre tal faceta.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se cargan las librerías iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el dataset de 'house pricing', este consiste en 80 variables (79 variables explicativas más una variable objetivo), describiendo aspectos fundamentales de hogares residenciales en la ciudad de Ames, Iowa. Este dataset está centrado en la regresión sobre el precio final de cada hogar. A continuación se procede a explorar tal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El conjunto a trabajar es el de entrenamiento\n",
    "df = pd.read_csv('data/train.csv', index_col = 'Id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cantidad de observaciones corresponde a 1460, por otra parte, posee 79 variables explicativas más un índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudia el tipo de valor y cantidad de información faltante para cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observado lo anterior y la estructura de la base, se aprecia que los datos de tipo `object` hacen referencia a 'strings' o categorias del dataset. Se crea una lista con aquellas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type_set = [col for col in df.columns if df[col].dtype == 'O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa la estructura del dataset en tales columnas, en este caso se decide transformarlas a formato 'str' para obtener visualizaciones sobre sus valores. El proceso de transformar los tipos de datos en un dataframe se conoce como *typecasting*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se transoforman las columnas anteriores a 'str'\n",
    "df = df.astype({col:'str' for col in object_type_set})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumado a lo anterior, se agregan niveles de multi indexado a las columnas para indicar si son del tipo numérico o categórico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['numeric', 'categorical']\n",
    "\n",
    "# Se crea una lista con las columnas numericas\n",
    "numeric = [\n",
    "    'LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n",
    "    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n",
    "    '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal', 'GarageYrBlt',\n",
    "    'MSSubClass','Fireplaces','SalePrice'\n",
    "]\n",
    "\n",
    "# Se crea una lista con las columnas categoricas\n",
    "categorical = list(set(df.columns) - set(numeric))\n",
    "''' \n",
    "Se generan mappings para el multi indexado del tipo \n",
    "\n",
    "[('numeric', col_if_numeric), ...,('categorical', col_if_categorical),...]\n",
    "'''\n",
    "\n",
    "mapping = [('numeric', col) for col in numeric]\n",
    "mapping.extend([('categorical', col) for col in categorical])\n",
    "'''\n",
    "Se reordenan las columnas del dataframe para que coincidan con el esquema \n",
    "del multi indice\n",
    "'''\n",
    "\n",
    "df = df.reindex(columns=numeric + categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalemente se asocia el multi indexado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se reasignan las columnas\n",
    "df.columns = pd.MultiIndex.from_tuples(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observación, se agrega que la columna 'MSSubClass' se clasifica como categórica pues representa un tipo de sector asociado a la propiedad. \n",
    "\n",
    "A continuación, se genera una visualización para entender la geometría de cada distribución, en el caso de las variables continuas, se calcula un estimado de la distribución por medio de `kernel density estimation`, este procedimiento consiste en elegir un tipo de función base (en este caso, una gaussiana con media en cada punto y de varianza constante) posteriormente, se calcula el promedio de las funciones base y se obtiene una función representante de la distribución denominada como 'density estimator', esto se hace por medio del método `sns.displot` de la librería `seaborn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=6, ncols=4, figsize=[17, 17])\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Numéricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), numeric):\n",
    "    try :\n",
    "        # Graficos para datos numericos\n",
    "        sns.distplot(df[('numeric', col)], ax=axis, rug=True)\n",
    "               \n",
    "    except RuntimeError:\n",
    "        sns.distplot(df[('numeric', col)], ax=axis, rug=True, kde=False)\n",
    "    \n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.4, .4)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las variables categóricas, se genera un conteo de valores únicos. Dado que se buscan las distribuciones de forma visual, se elimina información referente a las escalas, que dada la cantidad de gráficos a obtener, solo entorpece el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=10, ncols=6, figsize=[17, 17])\n",
    "\n",
    "# Se remueven los ultimos 3 plots\n",
    "list(map(lambda a : a.remove(), ax[-1,-3:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Categóricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), categorical):\n",
    "\n",
    "    # Graficos para datos tipos str\n",
    "    sns.countplot(df[('categorical',col)], ax=axis)\n",
    "    axis.set_axis_off()\n",
    "    axis.set_title(col, fontsize=15)\n",
    "  \n",
    "    \n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "h, w = (.4, .1)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar las distribuciones, es importante buscar si existe variabilidad dentro de estas, pues por lo general, una variable con un único valor casi seguro, no aporta información a la dinámica de los datos.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se observa la variable 'Heating' (categórica) y se compara con la variable de interés 'SalePrice'. Para ello se usa un gráfico de categórias tipo violín"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sirve para fija el tamaño de lasetiquetas del plot\n",
    "fontdict = {'fontsize':20}\n",
    "\n",
    "# Estrucutra de figura y axes\n",
    "fig, ax = plt.subplots(2,1,figsize=[12,13])\n",
    "\n",
    "# violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "sns.violinplot(('categorical', 'OverallQual'),\n",
    "            y=('numeric', 'SalePrice'),\n",
    "            data=df,\n",
    "            kind='violin',\n",
    "            ax=ax[0])\n",
    "\n",
    "sns.countplot(df[('categorical','OverallQual')], ax=ax[1])\n",
    "\n",
    "ax[0].set_xlabel('OverallQual', fontdict)\n",
    "ax[1].set_xlabel('OverallQual', fontdict)\n",
    "\n",
    "ax[0].set_ylabel('SalePrice', fontdict)\n",
    "ax[0].set_title('Violin plot OverallQuall vs SalePrice', fontdict)\n",
    "ax[1].set_title('Frecuencias OverallQuall', fontdict)\n",
    "\n",
    "h, w = (.3, .1)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un gráfico de violín permite sumarizar y observar características de un dataset. Este se comporta como un gráfico de cajas (boxplot), mostrando la mediana, el rango intercuantílico IQR (percentil 75 - percentil 25, o Q3 - Q1) y el rango 1.5 intercuantílico (Q3 +- 1.5 IQR). Además de lo anterior, se suma una estimación de la densidad por kernel a cada lado. Esto quiere decir, que zonas con mayor densidad, se verán como 'montes' horizontales. \n",
    "\n",
    "En el caso de 'OverallQuall', se ve una clara relación entre los distintos niveles de está variable en contraste con difierentes distribuciones de 'SalePrice'. Junto con una distribción que presenta variabilidad, se podría considerar como una de interés. \n",
    "\n",
    "\n",
    "Por otra parte, analizando las gráficas univariadas, se puede observar que para 'LandSlope', se tiene poca variablidad y no genera diferencias en distribución para 'SalePrice' en ninguna de sus categorias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sirve para fija el tamaño de lasetiquetas del plot\n",
    "fontdict = {'fontsize':20}\n",
    "\n",
    "# Estrucutra de figura y axes\n",
    "fig, ax = plt.subplots(2,1,figsize=[12,13])\n",
    "\n",
    "# violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "sns.violinplot(('categorical', 'LandSlope'),\n",
    "            y=('numeric', 'SalePrice'),\n",
    "            data=df,\n",
    "            kind='violin',\n",
    "            ax=ax[0])\n",
    "\n",
    "sns.countplot(df[('categorical','LandSlope')], ax=ax[1])\n",
    "\n",
    "ax[0].set_xlabel('LandSlope', fontdict)\n",
    "ax[1].set_xlabel('LandSlope', fontdict)\n",
    "\n",
    "ax[0].set_ylabel('SalePrice', fontdict)\n",
    "ax[0].set_title('Violin plot LandSlope vs SalePrice', fontdict)\n",
    "ax[1].set_title('Frecuencias LandSlope', fontdict)\n",
    "\n",
    "h, w = (.3, .1)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "\n",
    "1. Los gráficos generados anteriormente siguen exactamente el mismo patrón de generación, lo único que cambia es la columna a analizar. Esto es una mala práctica pues siempre se debe buscar reutilizar código o 'no repetirse' esto se conoce como principio DRY (don't repeat yourself). Construya una función que permita visualizar columnas categóricas del dataset y compararlas con 'SalePrice'. \n",
    "\n",
    "2. En función de las visualizaciones construidas, discuta que variables categóricas pueden ser de interés para predecir 'SalePrice'. Busque variabilidad y separación en la distribución de precios. ¿Qué ocurre si una variable categórica posee poca variablidad pero genera buenas separaciones en  'SalePrice'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comparar las variables numéricas, se pueden utilizar gráficos de dispersión contra 'SalePrice'. En este caso, se buscan variabilidad en el histograma univariado y a la vez, se buscan relaciones funcionales (del tipo lineal, exponencial, cuadrático, etc..) con 'SalePrice'. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Debido a su distribución, se estudia la variable 'GrLivArea', en este caso se define una función para gráficar variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_dists(col, df=df, h=.3, w=.1, fontdict={'fontsize': 20}, reg=True):\n",
    "    ''' Recibe una columna numerica y genera una visualizacion comparativa.\n",
    "    \n",
    "    Genera una figura por sobre el dataframe HousePricing (por defecto), recibe \n",
    "    parametros extra como el espaciado entre subfigura.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "    \n",
    "    col: String\n",
    "         El nombre de la columna numerica a visualizar\n",
    "    \n",
    "    h,w: float\n",
    "        Espaciado entre subplot h -> vertical, w -> horizontal\n",
    "    \n",
    "    fontdict: dict\n",
    "             Permite configurar las fuentes de los subplots\n",
    "    reg: bool\n",
    "         Permite graficar una regresion lineal sobre los datos (if True)\n",
    "        \n",
    "    Returns: None\n",
    "        Se muestra una figura en pantalla    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Estrucutra de figura y axes\n",
    "    fig, ax = plt.subplots(2, 1, figsize=[12, 13])\n",
    "\n",
    "    # violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "    if reg:\n",
    "        sns.regplot(x=df[('numeric', col)],\n",
    "                    y=df[('numeric', 'SalePrice')],\n",
    "                    ax=ax[0])\n",
    "        ax[0].set_title('Regplot plot {} vs SalePrice'.format(col), fontdict)\n",
    "    else:\n",
    "        sns.scatterplot(('numeric', col),\n",
    "                        y=('numeric', 'SalePrice'),\n",
    "                        data=df,\n",
    "                        ax=ax[0])\n",
    "        ax[0].set_title('Scatter plot {} vs SalePrice'.format(col), fontdict)\n",
    "\n",
    "    \n",
    "    # Distribucion univariada\n",
    "    sns.distplot(df[('numeric', col)], ax=ax[1])\n",
    "\n",
    "    ax[0].set_xlabel(col, fontdict)\n",
    "    ax[1].set_xlabel(col, fontdict)\n",
    "\n",
    "    ax[0].set_ylabel('SalePrice', fontdict)\n",
    "    ax[1].set_title('Frecuencias {}'.format(col), fontdict)\n",
    "\n",
    "    plt.subplots_adjust(wspace=w, hspace=h)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_dists('GrLivArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se puede observar una distrbución univariada bien definida y un comportamiento lineal aunque ruidoso. Esto hace que 'GrLivArea' sea una variable de interés. De la misma manera, '1stFlrSF', parece reflejar las mismas buenas características. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_dists('1stFlrSF') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de 'TotalBsmtSF' se tiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_dists('TotalBsmtSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una relación menos lineal con un poco más de ruido pero una buena distribución en e dataset. Esta variable puede ser de interés pero esto se puede estudiar a posteriori. \n",
    "\n",
    "Finalmente para 'MasVnrArea', se tiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_dists('MasVnrArea', reg = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia una distribución altamente concentrada y poco relacionada con la variable a predecir, a priori, se puede considerar como una variable de poco interés en el análisis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Estudie las siguientes proposiciones: \n",
    "\n",
    "    1.'OverallQual' y 'YearBuilt' parecen relacionadas con 'SalePrice'. \n",
    "    2. En el caso de 'OverallQual', esta relación es bastante débil.\n",
    "    3. En el caso de 'YearBuilt', esta relación es bastante débil.\n",
    "    4. Los gráficos de caja para 'OverallQual contra  'SalePrice' muestran cierta linealidad con respecto a 'SalePrice'.\n",
    "\n",
    "2. Estudie la distribución univariada de 'SalePrice', a continuación ejecute el test K^2 de D’Agostino usando `normaltest` del módulo `stats` de SciPy. Compare para una significancia de 5%. ¿ Se puede tratar esta variable como distribuida de manera normal, tomando en cuenta su comportamiento estadístico?\n",
    "\n",
    "3. Las distribuciones de 'TotalBsmtSF' y '1stFlrSF' parecen bastante similares, más aún sus relaciones con 'SalePrice' comparten una tendencia. Ejecute el [test de Kolmogorov-Smirnov ](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html) por medio de `ks_2samp` para explorar la hipótesis:\n",
    "\n",
    " ```'TotalBsmtSF' y '1stFlrSF' vienen de la misma distribución```\n",
    " \n",
    "4. Estudie algunos estadísticos de interés según el tipo de dato.\n",
    "    \n",
    "    1. Para las variables numéricas estudie promedios, desviaciones estándar y rangos intercuartílicos. Utilice los rangos calculados para tener una idea del porcentaje de valores fuera de tales rangos por columna. \n",
    "    \n",
    "    2. Para variables catégoricas calcule frecuencias, proporciones y modas. Utilice lo anterior para obtener alguna idea de la variabilidad de los datos.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfilamiento Bivariado\n",
    "\n",
    "Basándose en el perfilamiento anterior, es de utilidad observar relaciones entre variables de interés. Para esto se pueden emplear visualizaciones a pares. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se selecciona un conjunto de variables de interés y se investigan sus relaciones bivariadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera una función auxiliar\n",
    "\n",
    "def indexer(cols, t_c = df.columns):\n",
    "    '''Genera columnas multinivel a partir de nombres de columna planos.'''\n",
    "    \n",
    "    set_to_tuple = set(*[cols])\n",
    "\n",
    "    tuples = [\n",
    "        i for i in t_c if set_to_tuple.intersection(set(i))\n",
    "    ]\n",
    "    \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se selecciona un conjunto de variables a examinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest = [\n",
    "    'SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF',\n",
    "    'FullBath', 'YearBuilt'\n",
    "]\n",
    "\n",
    "idxs = indexer(interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[idxs].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a observar el comportamiento bivariado de las columnas seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pone SalePrice al final de la lista\n",
    "idxs.sort()\n",
    "idxs.remove(('numeric', 'SalePrice'))\n",
    "idxs.append(('numeric', 'SalePrice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Seaborn presenta problemas para multi indices en columnas, se \n",
    "procede a eliminar el nivel exterior y a obtener la visualización\n",
    "correspondiente.\n",
    "'''\n",
    "data = df.reindex(idxs, axis=1).droplevel(0,axis=1)\n",
    "sns.pairplot(data = data, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La última fila de la visualización anterior entrega una idea de la relación entre 'SalePrice' y las demás variables de interés. Dentro de estas relaciones, se observan ciertos comportamientos lineales y en particular para 'OveralQuall' y 'YearBuilt' se observa cierta exponencialidad. Dentro de las interacciones entre variables, se observa que 'GrLivArea' y 'TotalBsmtSf' se comportan de manera similar contra 'OverllQuall', esperandose cierta tendencia creciente en ambos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los análsis iniciales basados en visualizaciones sirven para comprender a grandes rasgos la estructura del dataset. Este tipo de exploración debe ser acompañada de tests estadísticos como los vistos en los ejercicios anteriores. En el caso del perfilamiento bivariado se puede usar una técnica mixta, basada en el análisis de las correlaciones.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se construye una matriz de correlaciones y se visualiza para todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se muestran las dos variables más correlacionadas (positivamente) con 'SalePrice'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = indexer(['SalePrice'])\n",
    "corrmat[col].nlargest(3,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a correlación negativa, no se ven relaciones lineales inversas de mayor fortaleza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat[col].nsmallest(3,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a visualizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Se inserta 'SalePrice' como primera fila x columna de la matriz de correlacion\n",
    "'''\n",
    "\n",
    "unsorted = list(corrmat.columns)\n",
    "unsorted.remove(*col)\n",
    "unsorted.insert(0, *col)\n",
    "\n",
    "sortd = pd.MultiIndex.from_tuples(unsorted)\n",
    "corrmat = corrmat.reindex(index = sortd, columns = sortd)\n",
    "'''\n",
    "Dado lo anterior, se ajusta el anchor de colores con maximo en .9\n",
    "y -0.5, para tener una perspectiva entorno a los valores maximos \n",
    "de correlacion (negativa y positiva)\n",
    "'''\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[16, 14])\n",
    "\n",
    "sns.heatmap(corrmat, vmin=-.5, vmax=.9, linewidths=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según el esquema de valores, se buscan los puntos más claros y más oscuros fuera de la diagonal. En primera instancia, las variables  'TotalBsmtSF' y '1stFlrSF' parece bastante correlacionadas, lo mismo ocurre con la variables 'GarageCars' y 'GarageArea', esto puede indicar multicolinearidad que implica información duplicada o relacionada de manera trivial en el dataset. \n",
    "\n",
    "Las correlaciones con 'SalePrice' deben ser analizadas con más detenimiento, aquí se ve que  'GrLivArea', 'TotalBsmtSF', y 'OverallQual' juegan un papel preponderante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Obtenga las 15 correlaciones más altas (positiva o negativa) con 'SalePrice'. Reindexe la matriz de correlaciones, de manera tal que contenga 1's en la diagonal y 'SalePrice' sea la primera fila - columna. \n",
    "\n",
    "2. Muestre los coeficiente de correlación dentro de cada casilla del gráfico de correlaciones. Utilice esa información en conjunción con el perfilamiento univariado para filtrar variables de interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las correlaciones pueden ser interpretadas con datos mixtos pero se recomienda analizar sus valores cuando se trabaja con valores continuos (comparación variable continua vs continua). Para analizar valores categóricos (categórico vs categórico) existen herramientas especializadas una de ellas es por medio de tablas de dos tratamientos o de contingencia (2 way tables). \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se construye una tabla para analizar 'OverallQual' vs 'GarageCars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare =['OverallQual','GarageCars']\n",
    "data_cat = df['categorical']\n",
    "\n",
    "kwargs = {'index': data_cat[to_compare[0]], 'columns': data_cat[to_compare[1]]}\n",
    "\n",
    "# Se construye la tabla\n",
    "tabla = pd.crosstab(**kwargs, margins=True, margins_name='Total')\n",
    "tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso anterior, la función `pd.crosstab(**kwargs, margins=True, margins_name='Total')` es equivalente a\n",
    "\n",
    "```python\n",
    "data_cat.pivot_table(**kwargs, values = 'OverallQual',aggfunc='count', fill_value=0)\n",
    "```\n",
    "y permite calcular el numero de ocurrencias de una variable para cada una de sus categorías en comparación con los valores de otra variable. Se añaden los totales como margenes de la tabla. En este caso, podemos deducir las interacciones entre las variables, de manera similar como actúa la correlación en variables continuas. \n",
    "\n",
    "Para el caso de  'OverallQual' y 'GarageCars' vemos que tienden a acumularse dentro de una rango reducido, se puede concluir que a medida que 'OverallQual' crece entre 4 y 6, aparece un aumento considerable en la categoría 'GarageCars' hasta que esta última llega al valor 2, valores superiores paracieran ser independientes de 'OverallQual'. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Compare variables categóricas usando este método, ¿ se puede encontrar alguna relación entre categórias?\n",
    "\n",
    "2. Es posible aplicar este método para comparar variables categóricas y continuas, para esto se necesita categorizar la variable continua objetivo. Categoríce la variable 'SalePrice' en 5 tramos y compare con 'OverallQual' ¿Se observa alguna tendencia?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de comparar variables categóricas es por medio de un test $\\chi^2$. Este permite obtener un indicador de significancia estadística entre variables. Se basa en una tabla de contingencia y proporciona la probabilidad de que dos variables categóricas sean independientes basádandose en el estádistico $\\chi^2$, entrega también un arreglo con frecuencias esperadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Se debe trabajar la tabla sin margenes\n",
    "tabla = pd.crosstab(**kwargs, margins=False)\n",
    "\n",
    "chi2, p, dof, ex =chi2_contingency(tabla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla de frecuencias esperadas se puede interpretar de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_freq = pd.DataFrame(ex, index=range(1,11))\n",
    "expected_freq.index.name = 'OverallQual'\n",
    "expected_freq.columns.name = 'GarageCars'\n",
    "expected_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, la frecuencia esperada para la la categoría 1 'OverallQual' de estar en la categoría 0 de 'GarageCars' es 0.11. Se puede decir que esta configuración es muy poco probable en comparación a otras como pertenecer a la categoría 6 de 'OverallQual' y 2 de 'GarageCars'. Este tipo de tablas permite clasificar las relaciones entre variables categóricas y obtener *insights* sobre las dinámicas que el dataset refleja. \n",
    "\n",
    "El valor $p$ entregado por el cáculo corresponde a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si para este test llamamos $\\alpha$ al valor de significancia, se puede resumir:\n",
    "\n",
    "1. Si $p > \\alpha$ no hay evidencia para rechazar la hipótesis nula por lo que se pueden considerar independientes.\n",
    "\n",
    "2. Si $p \\leq \\alpha$ hay evidencia para rechazar la hipótesis nula por lo que se puede decir que existe una dependencia estadística entre las variables. \n",
    "\n",
    "\n",
    "Para una significancia del 5% , hay evidencia para rechazar la hipótesis de independencia entre 'OverallQual' y 'GarageCars', luego puede existir un factor latente que las relaciona (¿será 'SalePrice'?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p <= 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como consideración general, para que este test sea consistente estadísticamente, se deben observar frecuencias (esperadas y observadas) mayores a 5. \n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Utilice el gráfico de correlaciones para escojer dos variables categóricas de interés. Verifique si existen relaciones estadísticas entre ellas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, para comparar variables numéricas y categóricas, es posible utilizar técnicas especializadas como lo son los tests Z y T. Estos tests se utilizan de manera simultanea con gráficos de caja (o violín), donde cada caja representa una categoría. \n",
    "\n",
    "Tanto el test Z como el T permiten verificar si las medias de dos grupos son estadísticamente diferentes entre si. aquí, el estadístico Z se define por\n",
    "\n",
    "\\begin{equation}z=\\frac{\\left|\\bar{x}_{1}-\\bar{x}_{2}\\right|}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}}}\\end{equation}\n",
    "\n",
    "Si su probabilidad asociada es pequeña, entonces **la diferencia de las medias** es significativa. \n",
    "\n",
    "Por otra parte, el estadístico T es más robusto a tamaños de observaciones pequeños (menores que 30 por ejemplo), este viene dado por \n",
    "\n",
    "\\begin{equation}\n",
    "t=\\frac{\\bar{X}_{1}-\\bar{X}_{2}}{\\sqrt{S^{2}\\left(\\frac{1}{N_{1}}+\\frac{1}{N_{2}}\\right)}}\n",
    "\\end{equation}\n",
    "\n",
    "Donde \n",
    "\n",
    "\\begin{equation}\n",
    "S^{2}=\\frac{\\left(N_{1}-1\\right) S_{1}^{2}+\\left(N_{2}-1\\right) S_{2}^{2}}{N_{1}+N_{2}-2}\n",
    "\\end{equation}\n",
    "\n",
    "Aquí , $\\bar{X}_{1}, \\bar{X}_{2}$ son las medias, $S_{1}^{2}, S_{2}^{2}$ varianzas y $N_1$ , $N_2$ los totales de cada grupo a testear. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Utilice el test de independencia $t$ (o 2 - sample $t$-test) para comparar 2 variables continuas de interés. [*Hint*](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html)\n",
    "\n",
    "2. Observe que el caso categórico vs continuo, cada categoría representa un grupo de valores continuos asociados. Por ejemplo, si la variable categórica `A` tiene las categorías `c_1` y `c_2`, al compararla con la variable continua `B`, es necesario agrupar los valores de `B` para `c_1` y para `c_2` para luego estudiar su independencia. Utilice el test anterior para medir independencias de grupos entre una variable categórica vs 'SalePrice'. **Obs**: La variable categórica debe ser bivariada.\n",
    "\n",
    "3. Utilice el test Z con las variables anteriores y compare. ¿Qué restricciones extra posee este test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se puede hacer uso de un test F o ANOVA. Este test permite comparar más de una media al mismo tiempo, una manera simple de aplicar este test consiste en método conocido como **one way ANOVA**, aquí, se testea si más de 2 grupos son similares basados en sus medias. En este caso, la hipótesis nula es \n",
    "\n",
    "`No hay diferencia significativa entre los grupos`\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se selecciona la variable 'GarageCars' y se compara con 'SalePrice'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = indexer(['SalePrice','GarageCars'])\n",
    "grouped = df[idx].groupby(idx[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la variable 'GarageCars' se distinguen 5 categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[idx[1]].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a partir de la agrupación anterior, se forman entonces 5 grupos de valores para 'SalePrice'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped.groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen los grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_groups = len(grouped.groups)\n",
    "groups = [grouped.get_group(i) for i in range(total_groups)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra el grupo correspondiente a la categoría 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se limpia el formato de cada grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_cleaner(group):\n",
    "    ''' Limpia un grupo.\n",
    "    Reconoce la categoria del grupo, en la posicion [:,1], \n",
    "    guarda ese nombre y elimina la columna de categoria, \n",
    "    posteriormente renombra la columna.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "    \n",
    "    group: pandas Groupby object\n",
    "          Recibe una agrupacion para categorias\n",
    "          \n",
    "    Returns:\n",
    "    ----------\n",
    "        pandas Grppuby object\n",
    "        Entrega el grupo ordenado.\n",
    "    '''\n",
    "    group_0 = group.copy()\n",
    "    name = group_0.iloc[0,1]\n",
    "    group_0.drop(indexer(['GarageCars']), axis=1, inplace=True)\n",
    "    group_0.columns  = ('cat_{}'.format(name),)\n",
    "    \n",
    "    return group_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se procede a limpiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_to_test = list(map(group_cleaner, groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra el grupo correspondiente a la categoria 0 post limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_to_test[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a testear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "F,p = f_oneway(*groups_to_test)\n",
    "\n",
    "print('Estadistico F:',F)\n",
    "print('p valor :', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probando para una significancia del 5% se tiene hay evidencia para rechazar la hipótesis nula y por tanto hay una diferencia significativa entre los grupos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "p <= alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Compruebe el resultado del test ANOVA anterior con un analísis visual por medio de gráficos de violín."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema de los datos faltantes \n",
    "\n",
    "Los métodos estándar de manejo de datos han sido desarrollados para para analizar arreglos tabulares. Por lo general las filas de tal arreglo representan observaciones y las columnas sus características asociadas. Cada entrada en este arreglo puede ser modelada como un número, siendo este ligado a un proceso subyacente continuo o discreto. Para comprender tal proceso, es de utilidad sumarizar y observar los valores faltantes con el fin de obtener patrones y seleccionar estrategias para tratarlos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de valores faltantes\n",
    "\n",
    "Cuando los datos faltantes se encuentran en variables que no son de interés, se pueden obviar y pasar a trabajar directamente en ingeniería de *features* e implementación de modelos de aprendizaje automático, por tal motivo, el análisis exploratorio y visualización se considera como primer paso en un procedimiento de análsis de datos. Sin embargo, una exploración preliminar de valores faltantes puede ser útil en conjunto con los perfilamientos visuales y estadísticos realizados.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Observe que en los perfilamientos anteriores, las variables categóricas:\n",
    "\n",
    "```python\n",
    "var_missing = ['GarageQual', 'GarageCond', 'BsmtFinType1','BsmtCond', 'GarageFinish', 'Fence', 'BsmtExposure',  'BsmtQual', 'MiscFeature', 'GarageType', 'Electrical', 'FireplaceQu', 'BsmtFinType2','MasVnrType']\n",
    "```\n",
    "Parecen no tener valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_missing = [\n",
    "    'GarageQual', 'GarageCond', 'BsmtFinType1', 'BsmtCond', 'GarageFinish',\n",
    "    'Fence', 'BsmtExposure', 'BsmtQual', 'MiscFeature', 'GarageType',\n",
    "    'Electrical', 'FireplaceQu', 'BsmtFinType2', 'MasVnrType'\n",
    "]\n",
    "\n",
    "var_missing = indexer(var_missing)\n",
    "\n",
    "df[var_missing].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, basta observar las columnas para comprender que tales variables si poseen valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[var_missing].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tal motivo es necesario realizar una exploración inicial de los datos faltantes en conjunción con los análisis de distribución iniciales. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Estudie la distribución de los valores faltantes en las variables numéricas.\n",
    "\n",
    "2. Considerando que para las variables categóricas las variables con valor 'nan' son consideradas como una nueva categoría. ¿Se ven afectados los análisis anteriores sobre sus distribuciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "Para estudiar en mayor profundidad la distribución de los valores faltantes, se procede a transformarlos en formato `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('nan',np.nan, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En términos generales, los valores perdidos de este dataset se encuentran relativamente limpios pues están estandarizados con la categoría 'nan'.\n",
    "\n",
    "\n",
    "Dado que sumarizar valores faltantes genera una estructura de datos, vale la pena explorarla visualmente, para facilitar tal tarea, existe la librería `missingno`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las visualizaciones de generadas por medio de esta librería pueden ser utilizadas para discutir el problema de valores faltantes y generan una estrategia para su tratamiento.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se genera una visualización sobre la distribución de valores perdidos en el dataset. En primer lugar, se confirma que la conversión 'nan' $\\mapsto$ `np.nan` sea reconocida en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en efecto aparecen los valores faltantes antes ignorados. En este apartado se observa que dentro de las variables categóricas se encuntra la mayor cantidad de información perdida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mediante la libreria `missingno` es posible ver el panorama completo de los valores faltanes en el dataset de manera sencilla "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = [15, 10])\n",
    "msno.matrix(df,ax = ax, sparkline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Genere un subconjunto con las 10 columnas con mayor información faltante y genere el gráfico anterior sin usar un objeto `axes` y con la opción `sparkline=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta visualización muestra que exiten columnas practicamente sin información, según la agregación anterior, estas corresponden a 'PoolQC', 'MiscFeature' y 'Alley'. \n",
    "\n",
    "Por medio de correlaciones entre valores faltantes, es posible obtener un análisis bivariado análogo al anteriormente generado. Para ello se puede utilizar un mapa de calor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = [15, 10])\n",
    "msno.heatmap(df, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico muestra correlaciones de nulidad entre pares de variables, estas varian desde -1 a 1, donde -1 significa que las variables son excluyentes, es decir, la aparición de una hace que la otra este ausente. Por otra parte el valor 1 corresponde inclusión, esto quiere decir, que la aparición de una hace que la otra aparezca. Valores cercanos a 0 (sin valor numérico en el gráfico) indican ausencia de relación de nulidad entre las variables.\n",
    "\n",
    "En el gráfico recien generado, no se observen relaciones de nulidad negativa, por otra parte, existen variables fuertemente relacionadas en cuanto a su información como lo son 'MasVnrType' y 'MasVnrArea', el comportamiento general es que la información esta fuertemente relacionada (en el sentido de inclusión de información) o simplemente no lo está. \n",
    "\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. El gráfico de correlaciones de nulidad permite tener una idea de como se relaciona la información faltante en pares de variables. Para comparar más de dos variables es posible utilizar un *dendograma*. Utilice las 20 variables con mayor cantidad de valores faltanes visualice su dendograma por medio de `msno.dendogram`. Interprete los resultados.[*Hint*](https://github.com/ResidentMario/missingno)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una perspectiva teórica\n",
    "\n",
    "Para dar un contexto teórico al problema de valores faltantes, se define la matriz de datos $Y=\\left(y_{i j}\\right)$, la cual representa un arreglo rectangular de datos. Para cada elemento de esa matriz, se asocia una variable indicadora, que da lugar a una *matriz indicadora de información faltante* definida por  \n",
    "$R=\\left(r_{i j}\\right)$. \n",
    "\n",
    "Por simplicidad, se puede asumir que las filas $\\left(y_{i}, r_{i}\\right)$ son i.i.d sobre $i$. Es posible así, modelar la existencia de un proceso (o fenómeno) que causa la perdida de información sobre $Y$ por medio de una distribución condicional de $R$  dado $Y$, denotada por $\\mathcal{P} \\left(R \\mid Y_{obs}, Y_{mis} ,\\phi \\right)$, donde $\\phi$ denota un conjunto de parámetros que modelan el proceso de perdida de información e $Y_{obs}$, $Y_{mis}$, denotan aquellas entradas de la matriz de datos $Y$ con información observada y faltante respectivamente. Si para tal proceso no se aprecia una relación con los valores faltantes en $Y$, es decir, \n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{eq:1}\n",
    "\\label{eq:1}\n",
    "\\mathcal{P} \\left(R \\mid Y_{obs}, Y_{mis} ,\\phi \\right)= \n",
    "\\mathcal{P} \\left(R \\mid \\phi \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Se dice entonces que el mecanismo de perdida de información es del tipo *información faltante completamente aleatoria* o MCAR (missing completely at random). Por otra parte, cuando la probabilidad de la información faltante en $Y$ se relaciona con variables observadas, así, $R$ depende de $Y_{obs}$ pero no de $Y_{mis}$, por lo que la distribución pasa a ser\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{eq:2}\n",
    "\\label{eq:2}\n",
    "\\mathcal{P} \\left(R \\mid Y_{obs}, Y_{mis} ,\\phi \\right)=\n",
    "\\mathcal{P} \\left(R \\mid Y_{obs}, \\phi \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Entonces, el proceso de perdida de información se denomina como, *información faltante aleatoria* MAR (missing at random). El proceso es llamado *información faltante no aleatoria* MNAR, si la distribución de $R$ depende de las componentes faltantes de $Y_{mis}$, es decir, la ecuación inicial ($\\ref{eq:2}$) no se cumple para algunas filas de $Y$ y algunos valores de las componentes faltantes.\n",
    "\n",
    "En los métodos discutidos en esta cátedra $R$ e $Y$ serán modelados por medio de distribuciones conjuntas, es decir, son tratadas como variables aleatorias. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "La estructura de datos con información faltante más simple, ocurre en el caso univariado. Acá, $Y$ y $R$ son vectores, luego \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{P}(Y=y, R=m | \\theta, \\phi)=\\prod_{i=1}^{n} f_{Y}\\left(y_{i} | \\theta\\right) \\prod_{i=1}^{n} f_{R \\mid Y} \\left(r_{i} | y_{i}, \\phi\\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde $f_{Y}\\left(y_{i} | \\theta\\right)$ denota la densidad de la componente $i$-sima de $Y$, $y_{i}$, parametrizada por $\\theta$, y $ f_{R \\mid Y} \\left(r_{i} | y_{i}, \\phi\\right)$ es la densidad de una variable aleatoria Bernoulli para el indicador binario $r_{i}$, con probabilidad $\\mathcal{Pr}\\left(r_{i}=1 | y_{i}, \\phi\\right)$ para $y_{i}$ valor faltante. Si el proceso de perdida de información es independiente de $Y$, es decir, $\\mathcal{Pr}\\left(r_{i} = 1 | y_{i}, \\phi\\right)=\\phi$ , constante que no depende de $y_{i}$, entonces el proceso de perdida de información es MCAR. Si tal mecanismo depende de $y_{i}$, entonces es MNAR pues pasa a depender de los valores perdidios de $y_{i}$.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Si se supone que $Y$ es una variable $n$ dimensional con posibles valores faltantes, $R$ es el indicador de pérdida de información para $Y$ y $X$ es una variable  $n$ dimensional relacionada al mismo dataset pero con valores completamente observados. En este contexto, si el dataset consiste de $n$ observaciones, donde para $r < n$  fijo, se tiene que $i=1, \\ldots, r$ $X_i$ e $Y_i$ son observados, mientras que para $j = r+1, \\ldots, n$ se tiene $X_j$ observado pero $Y_j$ faltante. \n",
    "\n",
    "1. Si para $i = 1, \\ldots, n$ se asume que $y_i$ es independiente de $r_i$ dado $x_i$. Aplique el supuesto MAR sobre este conjunto de datos y deduzca una expresión para $\\mathcal{P}(R \\mid X, Y , \\phi)$ que no dependa de $Y$. \n",
    "\n",
    "2. Utilice la expresión anterior para deducir que $R$ e $Y$ son independientes dado $X$. \n",
    "\n",
    "3. Utilice lo anterior para deducir que la distribución condicional de $Y$ dado $X$ y $R$ no depende de $R$. \n",
    "\n",
    "4. Deduzca que la distribución condicional de $Y$ dado $X$ puede ser estimada para componentes con $Y$ observado ($r_i = 0$) para luego ser utilizada para predecir valores faltantes de $Y$ ($r_i = 1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprender el significado de lo anterior, considere el siguiente ejemplo\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se realiza un experimento en cual se examinan ciertas variables relacionadas a la salud, dentro de estas variables, se consideran algunos comportamientos como *beber alcohol*, *fumar*, *consumo de drogas* y *actividad sexual*. El experimento se realiza sobre una población menor de 18 años, luego, por regulaciones gubernamentales, no se pueden realizar preguntas sobre actividad sexual a menores de 14 años. Observe\n",
    "\n",
    "1. La variable asociada al comportamiento sexual cumple la hipótesis MAR, pues en efecto, al observar la variable de edad (variable observada $Y_{obs}$), se puede comprender la falta de información subyacente.\n",
    "\n",
    "2. Por otra parte, suponga que la variable de edad y comportamiento sexual están altamente correlacionadas. Asuma además que en el estudio existe un valor de *índice de salud* asociado a cada participante. Los investigadores del experimento deciden utilizar regresión lineal sobre el valor del *índice de salud* para estudiar el peso de cada variable en el modelo. Al hacer esto, eliminan la variable de edad por producir multicolinearidad. \n",
    "Si los investigadores relacionan los valores del *índice de salud* con la probabilidad de que la variable sexual tenga valores faltantes, se está en la hipótesis MNAR, pues la variable edad pasa a ser no observada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interludio: Introducción a Scikit Learn\n",
    "\n",
    "Dentro la extensa variedad de librerías para el manejo de datos e implementacion de modelos, se encuentra [Scikit-learn](https://scikit-learn.org/stable/). Está librería es estándar en flujos de trabajo con datos y provee herramientas clásicas de aprendizaje automático implemententadas de manera eficiente. Sus APIs permite generar código limpio y está provista de una extensa documentación.\n",
    "\n",
    "\n",
    "Una gran ventaja de Scikit-learn consiste en su estructura transversal de clases, construida sobre una lista simple de APIs y patrones de diseño. Las Apis más representativas son:\n",
    "\n",
    "* *transformers*: Permite transformar datos input antes de utilizar algoritmos de aprendizaje sobre ellos. Con esto, se pueden realizar imputaciones de valores faltantes, estandrización de variables, escalamientos y seleccion de caracterísiticas por medio de algoritmos especializados.\n",
    "\n",
    "* *estimators*: La interfaz de esimadores es uno de los componentes más importantes. Los algoritmos de aprendizaje automático están implementados aquí. El proceso de aprendizaje de tales algoritmos es manejado según la inicialización de un objeto alojado en el módulo, esto consiste proporcionar los hiperparámetros que definen el modelo a entrenar, antes de proporcionar datos. El segundo paso corresponde a utilizar el método `.fit()` sobre los datos a utilizar, aquí se aprenden los parámetros y se encapsulan sus valores como atributos públicos para fácil inspección. \n",
    "\n",
    "* *predictors*: Esta interfaz permite generar predicciones usando un estimador previamente entrenado sobre datos (a priori) desconocidos. \n",
    "\n",
    "El método usual de importación se basa en seleccionar un submódulo de la librería indicando (de manera opcional) el objeto que se utilizará, por ejemplo, si se desea utilizar el escalador de datos Min-Máx del submódulo `preprocessing`, se haría de la manera usual, por medio de:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "**Obs**: No se recomienda importar la librería completa `import sklearn as sk` pues su estructura de submódulos es suficientemente grande, como para considerar cada uno como una librería. \n",
    "\n",
    "\n",
    "A lo largo del curso se estudiarán distintos componentes de esta librería, durante la siguiente sección nos centraremos en los móduloz `preprocessing`, `compose` y `pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos con Scikit-Learn\n",
    "\n",
    "El módulo `sklearn.preprocessing` entrega funciones de manejo de datos ampliamente utilizadas en la práctica. Hace uso de *transformers* con lo que facilita la transición de datos 'crudos' a un formato estándar para el entrenamiento de algoritmos. \n",
    "\n",
    "Dentro de tales algoritmos encontramos:\n",
    "\n",
    "#### Estandarización\n",
    "\n",
    "La estándarización es el primer tipo de transformación a tener presente, esto pues, una gran cantidad de algoritmos de aprendizaje automático / estadístico, asumen que los datos a operar se encuentran distribuidos de manera normal. \n",
    "\n",
    "En la práctica, se ignora la forma de la distribución a trabjar y simplemente e transforma removiendo la media y escalando por la desviación estándar.\n",
    "\n",
    "**Obs**:Esto **no** es recomendaddo si el histograma de la variable objetivo dista mucho de ser gaussiana, para tal caso se revisarán transformaciones alternativas.\n",
    "\n",
    "El objeto `StandarScaler` permite estandarizar datos.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se generan 3 distribuiones a estandarizar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.DataFrame({\n",
    "    'x1': np.random.normal(0, 2, 1460),\n",
    "    'x2': np.random.normal(10, 4, 1460),\n",
    "    'x3': np.random.normal(-15, 6, 1460)\n",
    "})\n",
    "df_0.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el escalador y se inicializa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan datos escalados, el método `.fit_transform` es transversal en los transformadores de `preprocessing`, lo que hace es obtener los parámetros de transformación de los datos proporcionados y transformar (todo en un paso). Es una abreviación de los métodos `.fit` para obtención de parámetros y `.transform()` para aplicar la transformación a nuevos datos. \n",
    "\n",
    "Por lo anterior, al aplicar `.fit_transform()` se obtienen los parámetros de media `.mean_` y desviación estándar `.scale_` para cada columna del dataframe operado. Observe que tales atributos del objeto tipo `StandardScaler` son públicos, observe además que es posible crear objetos que hereden de tal clase y por tanto, anular sus métodos utilizando métodos propios, recuerde que Python soporta Duck typing.\n",
    "\n",
    "Se obtienen los parámetros y se transforman las columnas generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = scaler.fit_transform(df_0)\n",
    "df_1 = pd.DataFrame(df_1, columns=['x1','x2','x3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se implementa la función `scaler_test` que simplifica el proceso anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_test(df, scaler, dat=False):\n",
    "    ''' Simplifica el proceso de testear transformadores de datos.'''\n",
    "    \n",
    "    data = df.copy()\n",
    "    data = scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(data, columns=df.columns)\n",
    "    data.plot.kde()\n",
    "\n",
    "    if dat:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalamiento mínimo-máximo\n",
    "\n",
    "Una buena alternativa al método anterior, es el escalamiento por rango, este tiene la forma:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_{i} - \\min(x)}{\\max (x)-\\min (x)}\n",
    "\\end{equation}\n",
    "\n",
    "para $x$ columna a tratar, $x_i$ elemento a transformar. Esta transformación permite hacer que los datos se muevan entre 0 y 1 y puede ser utilizado y la distribución de los datos a tratar no cumple la hipotesis de normalidad (recordar test Z). Observe que este transformador se ve afectado por la presencia de outliers. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se generan 3 distribuciones y se estudia la transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.DataFrame({\n",
    "    'x1': np.random.beta(8, 2, 1460)*100,\n",
    "    'x2': np.random.chisquare(10, 1460),\n",
    "    'x3': np.random.normal(50, 3, 1460)\n",
    "})\n",
    "\n",
    "df_0.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el objeto `MinMaxScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se prueba la transformación,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = scaler_test(df_0, scaler, dat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueban minimos y máximos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data min:', data.min())\n",
    "print('data max:', data.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea escalar por rengo, la mejor práctica es comprender los mínimos y máximos *absolutos* para cada columna. Esto se refiere, a las cotas superiores e inferiores que posee la columna **por definición**, a modo de ejemplo, considere un dataframe con las notas de una asginatura donde se enzeña análisis de datos, se sabe que la nota máxima en cierto ítem se codifica en una columna y su máximo es en efecto es 7.0, sin embargo el mínimo en dicha columna es 1.5, que es distinto al mínimo natural para dicho item que es 1.0. Esto puede acarrear problemas con datos nuevos, sobretodo si aparece una nota inferior a 1.5. \n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Investigue los parámetros que se deben usar para proporcionar escalamiento por rango con valores máximos y mínimos proporcionados explícitamente. \n",
    "\n",
    "2. Estudie el transformador `MaxAbsScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación robusta\n",
    "\n",
    "Cuando se trabaja con columnas que poseen valores fuera de rango (outliers) las transformaciones anteriores pueden fallar. En este caso, se recomienda utilizar una transformación de la forma\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_i - Q_1(x)}{IQR(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Donde $IQR = Q_3(x) - Q_1(x)$ es el rango intercuartílico de la columna $x$. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se generan datos con outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x1': np.concatenate([np.random.normal(20, 1, 1460), np.random.normal(1, 1, 25)]),\n",
    "    'x2': np.concatenate([np.random.normal(-10, 1, 1460), np.random.normal(50, 1, 25)]),\n",
    "})\n",
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el objeto `RobustScaler` y se aplica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "scaler_test(df, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar como los datos son centrados pero se mantienen los outliers generados.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Sea $k(x,y)$ un kernel definido por $\\langle \\phi(x), \\phi(y) \\rangle_{\\mathcal{H}}$, donde $\\phi: \\mathcal{X} \\rightarrow \\mathcal{H}$ es una fución que opera elementos entre espacio de datos $\\mathcal{X}$ y un espacio de Hilbert $\\mathcal{H}$. Se sabe por el *truco del kernel* que algoritmos kernelizados obvian el tratamiento de *features* no lineales representadas por transformaciones del tipo $\\phi(\\cdot)$. Sin embargo, es posible centrar datos en el espacio de carácteristicas $\\mathcal{H}$ utilizando la matriz de Gramm asociada a $k(x,y)$. Para efectuar tal procedimiento, estudie la clase `KernelCenterer`. ¿Que ventaja tiene usar este método?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapeo a distribuciones gaussianas \n",
    "\n",
    "Como mencionó anteriormente, no siempre se cumple la hipótesis de normalidad en las columnas de un dataset, en tal caso, no es una buena idea estandarizar los datos pues puede llevar a problemas al momento de operar con algoritmos que requieren normalidad en su formulación. Existe una familia de transformaciones paramétrica que busca aproximar una distribución arbitraria a una gaussiana, se accede a este tipo de transformaciones por medio de la clase `PowerTransformer`, en esta clase se encuentran 2 transformaciones:\n",
    "\n",
    "* Yeo-Johnson dada por: \n",
    "\n",
    "\\begin{equation}\n",
    "x_{i}^{(\\lambda)}=\n",
    "\\begin{cases}\n",
    "\\left[\\left(x_{i} + 1\\right)^{\\lambda}-1 \\right] / \\lambda & \\text { si } \\lambda \\neq 0, x_{i} \\geq 0 \\\\\n",
    "\\ln \\left(x_{i}+1\\right) & \\text { si } \\lambda=0, x_{i} \\geq 0 \\\\\n",
    "-\\left[\\left(-x_{i}+1\\right)^{2-\\lambda}-1\\right] /(2-\\lambda) & \\text { si } \\lambda \\neq 2, x_{i}<0 \\\\\n",
    "-\\ln \\left(-x_{i}+1\\right) & \\text { si } \\lambda=2, x_{i}<0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "* Box-Cox: Solo puede ser utilizada en datos extrictamente positivos. Viene dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "x_{i}^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\frac{x_{i}^{\\lambda}-1}{\\lambda} & \\text { si } \\lambda \\neq 0 \\\\\n",
    "\\ln \\left(x_{i}\\right) & \\text { si } \\lambda=0\n",
    "\\end{cases}.\n",
    "\\end{equation}\n",
    "\n",
    "En ambos casos, el parámetro $\\lambda$ es estimado por máxima verosimilitud.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "La técnica de transformación por medio de potencias (power transform) permite estabilizar la varianza y ahcer que los datos se distribuyan de manera más similar a la distribución normal. Por lo general, se recomienda el uso de este tipo de transformaciones en datasets con pocas observaciones pues generalizan de manera rápida, además son fácilmente interpretables por medio del valor $\\lambda$. Se definen distintas distribuciones de datos y se implementan las transformaciones anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "n_sample = 1460\n",
    "\n",
    "transformer_bc = PowerTransformer(method='box-cox')\n",
    "transformer_yj = PowerTransformer(method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un dataset con diferentes distribuciones por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x_lognormal': np.random.lognormal(size = n_sample,),\n",
    "    'x_chisq': np.random.chisquare(5, n_sample),\n",
    "    'x_weibull': np.random.weibull(30, n_sample),\n",
    "    'x_gaussian': np.random.normal(loc = 25, size = n_sample),\n",
    "    'x_uniform': np.random.uniform(0, 1, n_sample)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan las distribuciones univariadas para cada columna en la diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se transforman los datos usando los objetos anteriores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc = transformer_bc.fit_transform(df)\n",
    "df_bc = pd.DataFrame(df_bc, columns=df.columns)\n",
    "\n",
    "df_yj = transformer_yj.fit_transform(df)\n",
    "df_yj = pd.DataFrame(df_yj, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan los resultados para el método de Box - Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_bc, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan los resultados para Yeo-Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_yj, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Obtenga los valores de lambda para cada uno de los de métodos revisados.\n",
    "\n",
    "2. El preprocesamiento por transformación de cuantiles es un método robusto que permite transformar una distribución de datos en una variable uniforme o normal. Permite el reducir el impacto de outliers. Investigue su formulación, ventajas y desventajas, aplique el transformer `QuantileTransformer` en los datos recientemente generados para observar su comportamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalización\n",
    "\n",
    "Otro método de transformación de datos es la normalización de estos. Esto conisite en un mapeo a la bola cerrada según una norma a elección. Para la norma 'l2' en el espacio euclidiano de 3 dimensiones se tiene el normalizador\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_{i}}{\\sqrt{x_{i}^{2}+y_{i}^{2}+z_{i}^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "**Ejemplo** \n",
    "\n",
    "Se estudia como opera este transformador de datos en una visualización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x': np.random.randint(-50, 50, 1460),\n",
    "    'y': np.random.randint(-60, 60, 1460),\n",
    "    'z': np.random.randint(-70, 70, 1460)\n",
    "})\n",
    "\n",
    "# Permite generar graficos 3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=[12,9])\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(df.x, df.y, df.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudia el impacto de la transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer()\n",
    "df_1 = scaler.fit_transform(df)\n",
    "df_1 = pd.DataFrame(df_1, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se visualiza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,9])\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(df_1.x, df_1.y, df_1.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación de varaibles categóricas\n",
    "\n",
    "Para el manejo de adecuado de variables categóricas,  se recomienda expresar sus valores en función de códigos númericos. El transformer `OrdinalEncoder` permite transformar características categóricas en códigos enteros (números enteros)\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se define una variable categórica y se preprocesa según `OrdinalEncoder`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', index_col='Id')\n",
    "\n",
    "# Esta variable posee las categorias 'RL', 'RM', 'C (all)', 'FV' y 'RH'\n",
    "data = df['MSZoning'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se inicializa el codificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena el codificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se le da forma a los datos\n",
    "X = data.values.reshape([-1,1])\n",
    "\n",
    "# Se entrena el codificador\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen las categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = data.unique().reshape([-1,1])\n",
    "\n",
    "print('Codigos:')\n",
    "enc.transform(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueban los códigos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.Series(enc.transform(X).flatten(), name = 'codes', index=data.index)\n",
    "data_codes = pd.concat([data,codes], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa por ejemplo que el código 2, corresponde a 'RH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_codes.groupby('codes').get_group(2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación por variables dummy\n",
    "\n",
    "Un problema común con la códificación ordinal es que las variables pasan a ser consideradas continuas por algoritmos de machine learning (en especial por la API *estimators* de scikit-learn). Para evitar esto es posible convertir cada categoría en una columna por si sola y asignar un 1 cuando esté presente. \n",
    "\n",
    "Esto se puede llevar a cabo por medio del transformador ` OneHotEncoder`.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Utilice este transformador en los datos categóricos anteriores. Los resultados serán entregados en formato *sparse* por  lo que tendrá que hacer uso del método `.toarray()` de los arreglos de NumPy.\n",
    "\n",
    "2. Compare con la función `get_dummies` de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers definidos \n",
    "\n",
    "Si se desea realizar una transformación no estándar en los datos, es posible utilizar la clase `FunctionTransformer` y proporcionar una función de transformación.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se define un dataset y se opera con un transformador propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'cat_1': np.random.randint(30,80,size = 5),\n",
    "    'cat_2':range(5),\n",
    "     })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define un transformador propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def mapping(x):\n",
    "    x.copy()\n",
    "    x['cat_1'] += 5\n",
    "    x['cat_2'] = x['cat_2'].apply(str) + '+ 1'\n",
    "    \n",
    "    return x\n",
    "\n",
    "# se inicializa el transformador\n",
    "trasformer = FunctionTransformer(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica a los datos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trasformer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flujos de transformación con Pipelines y Compose\n",
    "\n",
    "Las transformaciones en un dataset son combinadas entre si, hasta obtener una versión ordenada de los datos, posteriormente, estas se combinan con estimadores para formar un flujo de trabajo *input-output*. En Sckit-Learn el flujo antes nombrado de denomina *composite estimator* y se construye por medio de objetos tipo `Pipeline`.\n",
    "\n",
    "\n",
    "Los objetos `Pipeline` se pueden utilizar para mezclar múltiples transformadores y estimadores generando un único tratamiento, su utilidad se expresa en la simplificación de etapas fijas en el tratamiento de datos, manteniendo un lenguaje sencillo e intuitivo. Se importan desde el módulo `pipeline` por medio de la convención\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "```\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utiliza una pipeline para transformar datos. En primera instancia se define un dataset por medio de las variables 'MSZoning' y 'LotArea' del dataset 'HousePricing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv',\n",
    "                 usecols=['Id', 'MSZoning', 'LotArea'],\n",
    "                 index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el dataset cargado, se efectúa una transformación ordinal sobre 'MSZoning', la variable 'LotArea' se escala según rango y se le aplica una transformación de Yeo-Johnson. El procedimiento mencionado, se lleva  a cabo mediante *pipelines*.\n",
    "\n",
    "En primer lugar, las series de pandas, se consideran objetos con dimensión del tipo `(n,)` al traspasarlos a formato NumPy. Para poder trabajar con transformers de sklearn, sobre series de pandas, podemos transormar la dimensión de la serie por meido dl método `.reshape([-1,1])` para que pase a ser considerada como arreglo de NumPy de dimensión `(n,1)`. Para automtizar tal proceso, se define la función `data_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_1d(df_column):\n",
    "    return df_column.values.reshape([-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se definen los transformadores que deseamos utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#reshape\n",
    "rs = FunctionTransformer(data_1d)\n",
    "\n",
    "#categorico\n",
    "ordinal = OrdinalEncoder()\n",
    "\n",
    "# Numericos\n",
    "numeric_1 =  MinMaxScaler() #minmax\n",
    "numeric_2 =  PowerTransformer(method='yeo-johnson') #yeo-johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se quisieran llevar a cabo los procesos de transformación, estos se harían de la forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Caso categorico'''\n",
    "\n",
    "data_cat = df.MSZoning.copy()\n",
    "data_cat = rs.fit_transform(data_cat)\n",
    "data_cat = ordinal.fit_transform(data_cat)\n",
    "data_cat = pd.Series(data_cat.flatten(), name = 'MSZoning')\n",
    "data_cat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Caso numerico'''\n",
    "\n",
    "data_num = df.LotArea.copy()\n",
    "data_num = rs.fit_transform(data_num)\n",
    "data_num = numeric_1.fit_transform(data_num)\n",
    "data_num = numeric_2.fit_transform(data_num)\n",
    "data_num = pd.Series(data_num.flatten(), name = 'LotArea')\n",
    "data_num.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La manera anterior es bastante clara de comprender, sin embargo, es redundante y repite muchos patrones de asiganción tediosos. Utilizando pipelines, es posible reducir la cantidad de código, sin reducir simpleza.\n",
    "\n",
    "Para definir la *pipeline* se debe definir una lista de tuplas, donde cada una posee un identificador (*nombre definido por el usuario*) y una operación a realizar sobre el dataset. en el caso de la variable 'MSZoning' se puede definir según el siguiente esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cat_pipe = Pipeline([('reshape', rs), ('ordinal', OrdinalEncoder())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otra parte, para la variable 'MSZoning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline([('reshape', rs), ('scaler', MinMaxScaler()),\n",
    "                     ('yeo-johnson', PowerTransformer(method='yeo-johnson'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los objetos tipo `Pipeline` permiten encadenar operaciones siguiendo el orden de la lsita de tuplas proporcionadas. En el caso de las pipelines proporcionadas, se efectúa primero (en ambas) la transformación `rs` que permite alterar la diemnsión de la series a operar. \n",
    "\n",
    "Si se desea solo obtener parámetros de transformación, se puede utilizar el método `.fit` sobre un objeto tipo `Pipeline`. Este actúa como un `map` aplicado a cada objeto transformador en cada tupla proporcioanada, de la misma forma, se pueden aplicar los métodos de transformación `transform` y de obteneción de parámetros con transformación directa `fit_transform`. \n",
    "\n",
    "En el caso anterior, esto corresponde a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe.fit(df.MSZoning)\n",
    "cat_pipe.transform(df.MSZoning)\n",
    "\n",
    "# Alternativamente cat_pipe.fit_transform(df.MSZoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe.fit(df.LotArea)\n",
    "num_pipe.transform(df.LotArea)\n",
    "\n",
    "# Alternaitvamente num_pipe.fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden utilizar los identificadores de cada paso (atributo `.steps`) para comprender las transformaciones de un objeto tipo `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pipe[0] for pipe in num_pipe.steps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se observa que las transformaciones numéricas corresponden a un reshape, luego a escalar los datos y finalmente a transformarlos según el método de Yeo-Johnson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Genere un trasformador personalizado que permita transformar los datos de una pipeline diseñana sobre arreglos unidimensionales en series de Pandas. La función a implementar debe recibir un arreglo y un nombre de columna, debe entregar una serie con los datos transformados cuyo nombre es el nombre de la columna procesada. Añada este último transformador a las pipelines `cat_pipe` y `num_pipe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy frecuente, es que los datos sea heterogéneos por ejemplo, es normal encontrar datasets con variables ordinales, categoricas, y numéricas. Para utilizar *pipelines* en este contexto, se necesitaria definir una por cada variable, repitiendo varios componentes de código entre variables que son del mismo tipo, esto resulta en una redundancia excesiva que se puede atacar por medio \n",
    "de objetos tipo `ColumnTransformer`, miembros del módulo `compose`. Estos objetos permite separar flujos de preprocesamiento, permitiendo seleccionar por columna o grupos de columna dentro de una *pipeline*.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utiliza un objeto tipo `ColumnTransformer` para tratar datos heterogéneos. En primera ligar se importan las transformaciones a realizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar en este ejemplo, se cargará la base 'Titanic', esta consiste en una base pequeña con características de pasajeros del Titanic y una variable categórica 1-0 que indica si sobreviveron o no al choque hundimiento. Se carga la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/titanic_train.csv', index_col='PassengerId')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos las columnas en numéricas y categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['Age','Fare']\n",
    "cat_cols = ['Embarked','Sex','Pclass']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya sabemos llenar valores faltantes con `.fillna()`, una alternativa es el uso de objetos `SingleImputer` del módulo `impute`. Este tipo de objetos permiten llenar valores faltantes de acuerdo a un método de agregación sobre los valores con información completa en las columnas de un dataset. Se usará la estrategia de llenado 'constant' que es equivalente a `.fillna()` pues llena todos los datos faltantes con el mismo valor proporcionado. \n",
    "\n",
    "Se procede a inicializar el objeto `SimpleImputer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan pipelines de preprocesamiento sobre las variables categóricas. Esta pipeline cambia todos los valores faltantes por 'NA' y luego genera una codificación Dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='NA')),\n",
    "    ('encoding', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las variables numéricas se genera la siguiente pipeline. esta llena los valores faltantes usando la mediana y luego estandariza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaling',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se combinan los procesos por medio de `ColumnTransformer`, este objeto se inicializa entregando una lista de tuplas, donde la primera componente es un identificador, la segunda una pipeline y la tercera un lista de strings contenedora de las columnas a tratar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipe, num_cols),\n",
    "        ('cat', cat_pipe, cat_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se aplican los procedimientos planificados en la variable `prep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene los datos transformados en formato NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Las pipelines pueden manejar transformadores de todo tipo y conectarlos entre si, observe que `ColumnTransformer` es un transformador por si solo, luego puede conectarlo con otro objeto por medio de una nueva pipeline. En particular, se pueden conectar transformadores con estimadores, estos ultimos son contenedores de rutinas de aprendizaje automático, que se entrenan por medio del método (transversal en la API *estimators*)`.fit()`. El objetivo de este ejercicio es que comience a utilizar estimadores en pipelines, para ello:\n",
    "\n",
    "1. Importe la clase `RandomForestClassifier` del submódulo `ensemble` de Scikit-Learn.\n",
    "2. Importe la función `train_test_split` del submódulo `model_selection` de  Scikit-Learn.\n",
    "3. Estudie la documentación de ambas clases, luego utilice `train_test_split` para generar conjuntos de entrenamiento y test, denominados `X_train`, `X_test`, `y_train`, `y_test`. En este caso la variable de respuesta `y` es la columna `'Survived'`. ¿Que porcentaje del conjunto de datos pasa a ser *test* por defecto?\n",
    "4. Inicialice un clasificador de *bosque aleatorio* (Random Forest) con 10 estimadores.\n",
    "5. Genere una pipeline que preprocese los datos según el esquema viston anteriormente y que como paso final entrene el clasificador Random Forest inicializado por usted. Esto se debe llavar a cabo por medio del método `.fit()`\n",
    "6. Estudie el porcentaje de aciertos del clasificador en datos test por medio del método `.score()` aplicado a la pipeline producida.\n",
    "\n",
    "**Obs**: Observe que para de pueden construir transformadores personalizados, haciendo uso de la clase `FunctionTransformer`. Si por otra parte se desea generar un transformador *desde 0* se puede crear una clase derivada de `BaseEstimator` y `TransformerMixin` (herencia múltiple, si). Para que la clase pueda actuar dentro de una pipeline, hace falta anular los métodos `fit`, `.transform` y `.fit_transform` (este último es opcional). Es un **buen** ejercicio generar un transformador usando este último método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manejo de valores faltantes\n",
    "\n",
    "Por lo general, existen razones prácticas y conceptuales a tener en cuenta cuando se trabaja con valores faltantes. \n",
    "\n",
    "En primer lugar, la falta de información introduce sesgos en los modelos de datos, pues hace que las muestras obtenidas no sean representativas del fenómeno que se desea estudiar, esto genera conclusiones sesgadas y puede llevar a tomar malas decisiones. \n",
    "\n",
    "En cuanto al componente práctico, los valores faltantes son incompatibles con algunos modelos de aprendizaje automático, debido a que estos modelos son parte de la razón fundamental de analizar un fenómeno por medio de datos,es que se necesita comprender bien los mecanismos de manejo de este tipo de valores. \n",
    "\n",
    "Según el contexto teórico anterior, el mecanismo de pérdida de información MCAR es el más sencillo en términos de modelación, pues solo requiere parametrizar la matriz indicadora de valores faltantes, sin considerar información fuera dentro de dataset. El test *MCAR de Little* sirve para probar si la información faltante en un dataset sigue la hipótesis MCAR. \n",
    "\n",
    "El test de Little evalúa diferencias en media entre subgrupos de datos con valores faltantes. Es una generalización del test-$t$ mencionado anteriormente. El estadístico de test es una suma ponderada según la ecuación:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{eq:3}\n",
    "\\label{eq:3}\n",
    "d^{2}=\\sum_{j=1}^{J} n_{i}\\left(\\hat{\\boldsymbol{\\mu}}_{j}-\\hat{\\boldsymbol{\\mu}}_{j}^{(\\mathrm{ML})}\\right)^{\\mathrm{T}} \\hat{\\mathbf{\\Sigma}}_{j}^{-1}\\left(\\hat{\\boldsymbol{\\mu}}_{j}-\\hat{\\boldsymbol{\\mu}}_{j}^{(\\mathrm{ML})}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Donde $n_j$ representa el numero de valores faltantes de la columna $j$. Dentro de la columna $j$ se generan grupos de valores faltantes en función de su relación con los valores faltantes de las demás columnas, así por ejemplo, el grupo 1 puede contabilizar solo aquellos valores faltantes para aquellas componentes (filas) presentes unicamente en la columna $j$ y que presentan información completa para todo $i \\neq j$, por otra parte, el grupo 2, puede poseer aquellos valores faltantes para cuya componente posee información faltante en la columna $i \\neq j$, pero inormación completa para todo $k \\neq i,j$. Se generan grupos hasta agotar las combinaciones. Luego, $\\hat{\\boldsymbol{\\mu}}_{j}$ representa un vector contenedor de medias para cada grupo, donde estas se medias, se calculan para las variables con información presente.\n",
    "$\\hat{\\boldsymbol{\\mu}}_{j}^{(\\mathrm{ML})}$ representa un contenedor de estimadores de medias para cada grupo por medio de máxima verosimilitud. Finalmente $\\hat{\\mathbf{\\Sigma}}_j$ represeta la matriz de covarianza entre cada grupo de la variable $j$.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "En el módulo `mcar` Se implementa el test de Little para cuantificar si un conjunto de datos posee un mecanismo de información faltante MCAR. Revise el código de la función `little_mcar`, comprenda su implementación.\n",
    "\n",
    "\n",
    "#### Eliminación\n",
    "\n",
    "Es el método más sencillo, se conoce tambien como 'list-wise deletion' y consiste en eliminar filas o columnas de un dataset que presenten datos faltantes. Se puede acceder a este tipo de tratamiento por medio de `.dropna()` objetos de Pandas.\n",
    "\n",
    "Otra forma de eliminación de datos se conoce como 'pair-wise deletion' que consiste en generar subconjuntos de un dataset en función de sus patrones de perdida de información. (Vea la implementación del test de Little). Posteriormente, se conducen análisis separados por patrón de perdida de información y se llegan a distintos modelos para cada uno.\n",
    "\n",
    "Ambos métodos son simples de implementar y asumen que el mecanismo de perdida de información en el dataset es del tipo MCAR. En casos distintos (MAR o MNAR) su uso es contraindicado. Se recomiendan cuando el patrón de perdida de información observada (por ejemplo por medio de `mssingno`) es claramente aleatorio, y si además las variables con información faltante son 'pocas' y con 'pocos' valores faltantes. La definición de 'poco' varia en función del problema, pero una buena huerística puede ser inferior al 15% en variables de poca importancia.\n",
    "\n",
    "#### Imputación\n",
    "\n",
    "Corresponde al llenado de información faltante por medio de estimaciones. Para efectuar este tipo de operaciones es importante tener en cuenta los mecanismos de pérdida de información latentes en los datos. Cabe destacar que el principal objetivo de la imputación no es maximizar la precisión (o maximizar metricas de similitud), sino que más bien, busca preservar las características del dataset inicial, generando uno con información completa que permita dilucidar las dinámicas implicadas en el fenómeno estudiado. \n",
    "\n",
    "Como resumen, los métodos de eliminación tienen más sentido bajo la hipótesis MCAR, luego si por ejemplo bajo el test de Little se adquiere un valor p > 0.05, cobra sentido evaluar tales estrategias.\n",
    "\n",
    "##### Imputación singular\n",
    "\n",
    "Como se ha venido aplicando a lo largo de este curso, corresponde a llenar valores faltantes con un valor único basado en la información observada, se requiere por tanto, tener evidencia de que el mécanismo de pérdida de información sigue la hipótesis MAR. \n",
    "\n",
    "Por lo general este tipo de imputación presenta un buen rendimiento empírico en tareas de ciencia de datos y es ampliamente recomendado, sin embargo, aplicar este tipo de métodos puede afectar el calculo de varianzas y covarianzas. \n",
    "\n",
    "En pandas podemos acceder a este tipo de imputación por medio del método `.fillna()` ya sea entregando un valor precalculado (media, mediana, moda, etc...) o utilizando los argumentos `ffill` y `bfill`. \n",
    "\n",
    "\n",
    "Existen directrices a tener en cuenta al momento de tratar valores faltantes:\n",
    "\n",
    "1. Valores faltantes en variables categóricas / ordinales:\n",
    "    * Transformar valores faltantes en una nueva categoría.\n",
    "    * Utilizar códificación Dummy en variables categóricas\n",
    "    * Agrear la categoría de valor faltante como orden inicial o final en categorias ordinales.\n",
    "\n",
    "\n",
    "2. Valores faltantes continuos:\n",
    "    * Probar métodos de imputación o eliminación.\n",
    "    * En mecanismos MCAR, utilizar media / mediana.\n",
    "    * En MAR o MNAR hay que tener en cuenta que la varianza no será representativa.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se desea aplicar el test de Little sobre el dataset 'HousePricing'. Para ello, se agrupan las variables de la base según el tipo de columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', index_col='Id')\n",
    "\n",
    "cat_cols = [\n",
    "    'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood',\n",
    "    'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n",
    "    'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n",
    "    'Heating', 'CentralAir', 'Electrical', 'GarageType', 'PavedDrive',\n",
    "    'MiscFeature', 'SaleType', 'SaleCondition'\n",
    "]\n",
    "\n",
    "ordinal_cols = [\n",
    "    'LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual',\n",
    "    'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n",
    "    'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual',\n",
    "    'GarageCond', 'PoolQC', 'Fence'\n",
    "]\n",
    "\n",
    "# Adquieren las categorias de cada variable\n",
    "ordinal_cat = [['Reg', 'IR1', 'IR2', 'IR3'],\n",
    "               ['AllPub', 'NoSewr', 'NoSeWa', 'ELO'], ['Gtl', 'Mod', 'Sev'],\n",
    "               ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'No', 'Mn', 'Av', 'Gd'],\n",
    "               ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "               ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "               ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd',\n",
    "                'Ex'], ['NA', 'Unf', 'RFn', 'Fin'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "               ['NA', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']]\n",
    "\n",
    "num_cols = [\n",
    "    'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
    "    'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
    "    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
    "    'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
    "    'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
    "    'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
    "    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
    "    'MoSold', 'YrSold'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores perdidos en las variables categóricas serán completados con la nueva categoría 'NA', luego se procesan según una codificación Dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Pipeline categorica\n",
    "cat_pipe = Pipeline(\n",
    "    steps=[('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')), \n",
    "           ('onehot',OneHotEncoder(sparse=False, handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos numéricos serán estandarizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Numerica\n",
    "num_pipe = Pipeline(steps=[('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores ordinales serán tratados de la misma manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Ordinal\n",
    "ord_pipe = Pipeline(\n",
    "    steps=[('imputer_ord', SimpleImputer(strategy='constant', fill_value='NA')),\n",
    "           ('ordinal', OrdinalEncoder(categories = ordinal_cat))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se componen las pipelines generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocesador Compuesto\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[('num', num_pipe, num_cols), \n",
    "                  ('cat', cat_pipe, cat_cols), \n",
    "                  ('ord', ord_pipe, ordinal_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que bajo este tratamiento, las variables ordinales y categóricas tendrán una categoría extra que aportará información al modelo y que por tanto no será contabilizada por el test de Little.\n",
    "\n",
    "Es **importante** que observe que el preprocesamiento fue realizado sobre las variables regresoras (no la dependiente), pues al momento de recibir nuevos datos, serán las columnas asociadas a variables regresoras,las que vendrán con información.\n",
    "\n",
    "Se preparan los datos según las transformaciones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables regresoras\n",
    "X = df.drop('SalePrice', axis=1).copy()\n",
    "\n",
    "# Variable dependiente\n",
    "y = df['SalePrice'].copy()\n",
    "\n",
    "# Se preparan los datos\n",
    "X_prep = prep.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que para las variables categóricas, se generan nuevas columnas asociadas a sus categorías. Se puede acceder a tales nuevas columnas por medio del atributo publico `.named_transformers_` del preprocesador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtienen las variables categoricas transformadas\n",
    "post_cat = prep.named_transformers_['cat'][-1]\n",
    "cat_cols_fit = post_cat.get_feature_names(cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las variables numéricas y ordinales, no hay cambios en las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnas del datase luego de transformarlo\n",
    "post_cols = list(num_cols) +list( cat_cols_fit) + list(ordinal_cols)\n",
    "len(post_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye el dataset transformado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = pd.DataFrame(data=X_prep, columns=post_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dimension del nuevo dataset es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se Aplica el test de Little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcar import little_mcar\n",
    "little_mcar(df_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el valor $p$ obtenido se tiene que las diferencias en medias producidas por los patrones de datos perdidos, son **inconsistentes** con mecanismo MCAR. Por lo tanto, no se recomienda utilizar metodos de eliminación de valores faltantes, a menos que la la columna donde se encuentren estos valores sea de poca significancia en comparación a la variable dependiente.\n",
    "\n",
    "\n",
    "**Ejercicios**\n",
    "\n",
    "1. Compare la `get_dimmies` de pandas con el método one\n",
    "\n",
    "2. El módulo `impute` de Scikit - Learn permite hacer imputaciones singulares por medio de la función (antes implementada) `SimpleImputer`. Debido a que el dataset 'HousePricing' con alta probabilidad no sigue un mecanismo de perdida de información MCAR, es posible asumir que sigue un patrón MAR (o MNAR). En este último caso, podemos asumir que los valores faltantes se pueden estimar por medio de los presentes.\n",
    "\n",
    "    1. Utilice `SimpleImputer` para llenar los valores faltantes de 'LotFrontage' en el dataset `df_post`.\n",
    "\n",
    "    2. Utilice el estimador `KNNImputer` para llenar los demás valores faltantes. **Obs**: Este método corresponde a llenar valores faltantes por medio de KNN, deberá elegir el número de vecinos para la imputación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que en general puede utilizar estimadores para llenar valores faltantes, un ejemplo común es el uso de interpoladores. en términos generales, se puede utilizar un modelo sobre los datos con información completa para imputar valores faltantes. Estos métodos de imputación flexible pueden ser fácilmente implementados y se conoce como **métodos de imputación multivariada**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación múltiple\n",
    "\n",
    "Un problema de la imputación singular es que modela los datos como uno completo, sin considerar la incertidumbre inherente de los datos. Los métodos de imputación múltiple permite generar estadísticos insesgados para imputar y considerar incertidumbre en la imputación. Este método es el único lo suficientemente robusto para manejar datos faltantes fuera de la hipótesis MCAR. Su proceso de calculo se resume en:\n",
    "\n",
    "1. Escoger un modelo de imputación uni-o multi -variado.\n",
    "2. Generar $m$ datasets completos con los métodos elegidos anteriormente.\n",
    "3. Analizar cada dataset generado aplicando la técnica de modelación que el problema requiere (regresión lineal, SVM, etc...) contra la variable de respuesta\n",
    "\n",
    "4. Analizar los efectos de imputación de valores perdidos sobre los estimadores utilizados.\n",
    "\n",
    "\n",
    "El objeto `IterativeImputer` del módulo `impute` permite utilizar el método de imputación múltiple basado en ecuaciones encadenadas [MICE](https://www.jstatsoft.org/article/view/v045i03). A diferencia del método antes descrito, retorna imputaciones singulares por variable uni- o muti- variadas. \n",
    "\n",
    "Su uso es experimental y se activa siguiendo la sintaxis:\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "```\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se utiliza el imputador iterativo para los estimadores de Random Forest y KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=0, n_estimators=5)\n",
    "knn = KNeighborsRegressor(n_neighbors=8)\n",
    "\n",
    "imputer_RF = IterativeImputer(estimator=rf,\n",
    "                              skip_complete=True,\n",
    "                              verbose=1,\n",
    "                              random_state=1)\n",
    "\n",
    "imputer_KNN = IterativeImputer(estimator=knn,\n",
    "                              skip_complete=True,\n",
    "                              verbose=1,\n",
    "                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_RF.fit_transform(df_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_KNN.fit_transform(df_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio** (Opcional)\n",
    "\n",
    "1. Investigue sobre las reglas de Rubin para imputación múltiple. Utilice el esquema de cuatro pasos definido anteriormente y aplique las reglas para analizar la sensibilidad imputación - predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección y manejo de Anomalías\n",
    "\n",
    "Una anomalía (*outlier* en ingles) es un dato significativamente distinto a los demás. Se puede considerar como una observación cuya desviación del conjunto de datos permite establecer la hipótesis, de que su generación fue obtenida por un mecanismo distinto al principal en la modelación de un fenómeno.\n",
    "\n",
    "Las anomalías contienen por tanto información sobre características anormales de las entidades y esquemas que impactan el proceso generativo de los datos. Reconocer estas observaciones permite, desde el punto de vista teórico, mejorar el entendimiento de los problemas modelados. Desde el punto de vista práctico, permite mejorar procesos de adquisición de datos y presición de modelos. En este último capitulo, se estudian algunas técnicas de detección de anomalías.\n",
    "\n",
    "\n",
    "\n",
    "### Métodos de manejo de anomalías\n",
    "\n",
    "Se revisan algunas técnicas de detección de outliers revisando su formulación.\n",
    "\n",
    "#### Desviación Estándar\n",
    "\n",
    "Si se estima que la variable a estudiar e distribuye de manera normal, entonces el 95% de los datos se encuentra a 2 desviaciones estándar de la media, mientras que el 99.7% se encuentra dentro de 3 desviaciones estándar. Basándose en esto, se puede considerar que cualquier punto fuera de 3 desviaciones estándar de la media como candidato a anomalía. Una forma más flexible de estimar anomalías usando el principio de normalidad, es por medio de z-scores. \n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se genera un dataset con valores anomalos y se estudian sus estadisticos z asociados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "data = np.random.normal(size=1000)\n",
    "data[-5:] = [3.5,3.6,4,3.56,4.2]\n",
    "data = pd.Series(data, name = 'test z')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calculan los z-scores y se comparan con un valor umbral, en este caso 3. Se puede flexibilizar tal valor con tal de explorar distintos rangos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[np.abs(zscore(data)) > 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se va a estudiar una columna con ouliers mediante este método, es coveniente hacer un test de normalidad. Si la variable no cumple con la hipótesis, es posible preprocesarla usando el método de box-cox, Yeo-Johnson o Inter Quantilico. Se recomienda este último por ser robusto a outliers. \n",
    "\n",
    "**Obs**: para esta formulación, una anomalía es un punto *fuera de rango* si bien son definiciones similares, por lo general son distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR: Rango intercuantílico\n",
    "\n",
    "Si se desea analizar datos univariados que no siguen una distribución normal, se puede estudiar su rango intercuantílico.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se genera un dataset con outliers y se estudia este método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "data = np.random.beta(2,3,size=10000)\n",
    "data[-5:]=[-0.3,1,1.2,0.9,2]\n",
    "\n",
    "data = pd.Series(data, name = 'test iqr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calcula el rango intercuantílico y se definen cotas superiores e inferiores en función de su valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_val = iqr(data)\n",
    "\n",
    "cota_inf = np.percentile(data,25) - iqr_val*1.5\n",
    "cota_sup = np.percentile(data,75) + iqr_val*1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudian los datos fuera de rango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data < cota_inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data > cota_sup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una manera visual de comprobar este método (y más rápida) es por medio de gráficos de caja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = [6,5])\n",
    "data.plot.box(ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el rango intercuantílico se utiliza para medir la dispersión de los datos, esto se logra separandolos en cuartiles (cuatro intervalos). La diferencia entre el primer cuartil y el tercero es el IQR (=Q3 - Q1). En el gráfico de caja, vemos que los outliers están sobre y bajo las lineas rectas, cada una representa Q1 - 1.5 x IQR (linea inferior) y Q3 + 1.5 x IQR (linea superior) Los valores dentro de la caja corresponden al IQR y la linea central es la mediana de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de mezcla de gaussianas\n",
    "\n",
    "El modelo de mezcla de Gaussianas corresponde a un modelo para clusterización de muestras que obedece un enfoque generativo. Este modelo puede ser pensado como una generalización del modelo K-Means ya que permite que los clusters encontrados tengan forma anisotrópica. Este modelo permite mayor flexibilidad. Se estudia su formulación.\n",
    "\n",
    "Sea $X = \\left \\{ x_i \\right \\}_{i=1}^N \\subset \\mathbb{R}^L $. El modelo de mezcla Gaussiano tiene como supuesto principal el hecho que los datos observados son muestras *i.i.d* de la distribución $p(x)$, la cual tiene la siguiente forma:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)\n",
    "\\end{equation}\n",
    "\n",
    "como se observa la distribución $p$ depende de varios parámetros y consiste en un modelo ideado para aproximar distribuciones como la suma de $K \\in \\mathbb{N}$.\n",
    "\n",
    "- El hiper parámetro $K \\in \\mathbb{N}$ corresponde a la cantidad de clusters, este debe ser predefinido.\n",
    "- $\\pi_k \\geq 0$ corresponde a la proporción de mezcla asociada la cluster $k$, se cumple que $\\sum_{k=1}^K \\pi_k = 1$.\n",
    "- $\\mu_k$ es la media de la Gaussiana asociada al cluster $k$.\n",
    "- $\\Sigma_k$ es la matriz de covarianza de la Gaussiana asociada al cluster $k$.\n",
    "\n",
    "Adicionalmente la variable latente $z = (z_1,\\ldots,z_K) \\in \\{ 0,1 \\}^K$ indica a cual de los $K$ clusters pertenece la observación $x$. Si $z_k = 1$ entonces $x$ pertenece al cluster $k$, solamente 1 de los elementos de $z$ es distinto a 0.\n",
    "\n",
    "La probabilidad de que una muestra pertenezca al cluster $k$ puede escribirse como:\n",
    "\n",
    "\\begin{equation}\n",
    "p(z_k = 1) = \\pi_k\n",
    "\\end{equation}\n",
    "\n",
    "por lo tanto la probabilidad conjunta de $z$ es:\n",
    "\n",
    "\\begin{equation}\n",
    "p(z) = \\prod_{k=1}^K \\pi_k^{z_k}\n",
    "\\end{equation}\n",
    "\n",
    "Por otro lado se puede escribir la probabilidad condicional de $x$ dado que pertenece al cluster $k$:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x|z_k = 1) = \\mathcal{N}(x|\\mu_k, \\Sigma_k)\n",
    "\\end{equation}\n",
    "\n",
    "En consecuencia la distribución condicional de $x$ dado $z$ es:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x|z) = \\mathcal{N}(x|\\mu_k, \\Sigma_k)^{z_k}\n",
    "\\end{equation}\n",
    "\n",
    "Se procede a calcular la distribución posterior de $z_k$:\n",
    "\n",
    "\\begin{equation}\n",
    "p(z_k = 1 | x) = \\frac{p(x|z_k = 1)p(z_k = 1)}{p(x)} = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)}\n",
    "\\end{equation}\n",
    "\n",
    "este termino se conoce como responsabilidad de la componente $k$ por la observación $x$. Se define $\\gamma_{z_k}(x) = p(z_k = 1 | x)$.\n",
    "\n",
    "Con una expresión para la distribución posterior y recordando el hecho que las muestras $X$ son *i.i.d*, se procede a calcula la log-verosimilitud de la muestra:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(x_1, \\ldots, x_N) = \\sum_{n=1}^N \\log p(x_n) = \\sum_{n=1}^N \\log \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_n, \\mu_k, \\Sigma_k)\n",
    "\\end{equation}\n",
    "\n",
    "La forma de esta verosimilitud no es tán simple de optimizar, se presentan complicaciones para manejar la suma dentro del logaritmo, por esta razón se utilizará un enfoque alternativo a máxima verosimilitud.\n",
    "\n",
    "El algoritmo que suele ser usado para estimar los parámetros de la posterior es conocido como algoritmo EM (Expectation maximization), sus etapas se exponen a continuación:\n",
    "\n",
    "- **Inicialización:** Se inicializan los parámetros $\\mu_k^0$, $\\Sigma_k^0$ y $\\pi_k^0$ $\\forall k =1,\\ldots,K$ ; medias, matrices de covarinza y porciones de mezcla respectivamente. Luego se evalua el valor de la log-verosimilitud.\n",
    "\n",
    "\n",
    "- **Paso E:** Se evaluan los terminos de responsabilidad utilizando los parámetros actuales\n",
    "\n",
    "$$ \\gamma_{z_{nk}}(x_n) = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)}{\\sum_{k=1}^K \\pi_k \\mathcal{N}(x_n|\\mu_k, \\Sigma_k)}$$\n",
    "\n",
    "\n",
    "- **Paso M:** Se re estiman los parámetros del modelo con los nuevos valores de responsabilidad encontrados utilizando las siguientes expresiones (condiciones de primer orden para optimalidad):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu_k = \\frac{\\sum_{n=1}^N \\gammạ_{z_{nk}}(x_n)x_n}{\\sum_{n=1}^N \\gammạ_{z_{nk}}(x_n)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\Sigma_k = \\frac{\\sum_{n=1}^N \\gammạ_{z_{nk}}(x_n)(x_n-\\mu_k)(x_n-\\mu_k)^T}{\\sum_{n=1}^N \\gammạ_{z_{nk}}(x_n)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_k = \\frac{1}{N} \\sum_{n=1}^N \\gammạ_{z_{nk}}(x_n)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- **Evaluación:** Evaluar la log-verosimilitud con los nuevos parámetros. Si no hay convergencia (necesario definir un criterio de convergencia) repetir desde el **paso E**.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se procede a estudiar una aplicación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "\n",
    "# Datos aleatorios\n",
    "np.random.seed(0)\n",
    "\n",
    "C = np.array([[0., -0.1], [1.7, .4]])\n",
    "C2 = np.array([[1., -0.1], [2.7, .2]])\n",
    "\n",
    "X = np.r_[np.dot(np.random.randn(n_samples, 2), C),np.dot(np.random.randn(n_samples, 2), C2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agregan puntos fuera de rango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[-5:] = [[4,-1],[4.1,-1.1],[3.9,-1],[4.0,-1.2],[4.0,-1.3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se grafican los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,5])\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el modelo y se aplica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizan las asignaciones de cluster para analizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,5])\n",
    "pred = gmm.predict(X)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1],c=pred, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo general, para seleccionar el número de componentes de la mezcla, es necesario tener conocimiento del problema de fondo \n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "1. Ejecute el modelo de mezcla de gaussianas para 3 componentes. Extraiga los potenciales outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos basados en densidad\n",
    "\n",
    "Estos modelos aproximan la distribución de los datos a analizar asinando densidades (paremétricas o no paramétricas) a sus puntos. Obtienendo tal estimador, se puede tener una idea de la probabilidad de que un punto del dataset sea un outlier\n",
    "\n",
    "### KDE: Estimador de densidad por kernel\n",
    "\n",
    "Si $(x_1, \\ldots, x_n)$  corresponden a observaciones univariadas iid obtenidas de alguna distribución desconocida. Entonces se puede estimar la densidad $f$ asociada a tal distribución por medio de $\\hat{f}_{h}$ dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_{h}(x =\\frac{1}{n h} \\sum_{i=1}^{n} K\\left(\\frac{x-x_{i}}{h}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Donde $K$ es una función no negativa conocida como *kernel* y $h >0$ es un parámetro de suavidad denotado como *ancho de banda*.\n",
    "\n",
    "**Obs**: La definición aquí presentada hace referencia a kernels invariantes a reotación como el kernel gaussiano o RBF, en particular, la elección del kernel puede ser cualquier función no negativa y simétrica que tenga sentido como función base.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "A continuación se explora el método KDE para la extracción de outliers. Primero, se genera un dataset y luego se explora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "n = 1460\n",
    "d = 3\n",
    "\n",
    "eje = 6\n",
    "\n",
    "X = np.zeros((n, d))\n",
    "\n",
    "X[:int(n/2),0] = eje\n",
    "X[(-int(n/2)+1):,0] = -eje\n",
    "\n",
    "X += np.random.normal(0,1, size = X.shape)\n",
    "\n",
    "sample = np.random.randint(n, size = int(n/100)) \n",
    "\n",
    "X[sample] +=  0.3*np.random.normal(0,5, size = (int(n/100),d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a importar el método y a aplicarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "kde = gaussian_kde(dataset=X.T)\n",
    "\n",
    "scores = kde.evaluate(X.T)\n",
    "idx = scores.argsort()\n",
    "scores.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[6,5])\n",
    "\n",
    "plt.bar(range(50),scores[:50])\n",
    "plt.title('Outlier score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad de potenciales outliers:',len(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el estimador kde estima la densidad asociada a la distribución de los datos, es posible asignar un puntaje a cada dato basándose en cuan baja es su densidad asociada por medio del kde entrenado. En el caso anterior se muestran los 50 puntajes más bajos. El arreglo de datos asociados a tales puntajes está en la variable `idx`. Con lo anterior, es posible seleccionar varaibles de acuerdo a su puntaje y relación con la columna de respuesta a modelar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede obtener una visualización del fenómeno anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "fig = plt.figure(figsize=[8, 5])\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=scores, linewidth=0.5, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Investigue que kernel es usado por defecto en el ejemplo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de densidad por medio de KNN\n",
    "\n",
    "Una manera sencilla de estimar densidades (y de manera no paramétrica) es por medio del método KNN. En esta caso, la densidad asociada a un punto $x$ vienen dada por $1/\\bar{N_k}(x)$ donde $\\bar{N_k}(x)$ es el promedio de los vecinos para $x$.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se inicializa el algoritmo para trabajar con 100 vecinos, luego se entrena en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "K=100\n",
    "knn = NearestNeighbors(n_neighbors=K)\n",
    "knn.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada punto de `X` encontramos sus `K` vecinos mas cercanos, se almacenan en un arreglo de $K \\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, _ = knn.kneighbors(X,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La densidad asociada a un punto sera el inverso del promedio de sus vecinos. Se calcula tal densidad para todos los puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_density_est = K/D.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualiza el experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6,5])\n",
    "\n",
    "scores = pd.DataFrame(data = knn_density_est, index = range(n))\n",
    "scores.sort_values(by=0, inplace=True)\n",
    "\n",
    "plt.title('Outlier score')\n",
    "scores[:25].plot.bar(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[8, 5])\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2], c=knn_density, cmap='viridis', linewidth=0.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering DBScan \n",
    "\n",
    "DBScan: Density based spatial clustering of applications with noise, es un algoritmo de clustering ampliamente utilizado. Una de sus principales fortalezas es que depende de solo 2 hiper-parámetros y no requiere definir una cantidad de clusters de antemano.\n",
    "\n",
    "Como su nombre lo indica este es un algoritmo que se apoya en nociones espaciales, por lo tanto requiere de una métrica para comparar la distancia entre puntos. En la mayoria de las aplicaicones se suele usar la norma euclidiana para estos propositos.\n",
    "\n",
    "Los parámetros del modelo son:\n",
    "\n",
    "- Radio de vecindad $\\epsilon$\n",
    "- Cantidad mínima de puntos en la vecindad $N_{min}$\n",
    "\n",
    "para el resto del extracto estos se consideraran dados y fijos.\n",
    "\n",
    "Sea $X = \\left \\{ x_i \\right \\}_{i=1}^N \\subset \\mathbb{R}^M $ una colección de puntos. El algoritmo DBSCAN se apoya sobre 3 nociones, que dependen de  las cuales se definen a continuación:\n",
    "\n",
    "- **$\\epsilon$-vecindad:** Sea $x \\in X$, se define el conjunto\n",
    "        \n",
    "    $N_{\\epsilon}(x) = \\left \\{ y \\in X \\hspace{2mm}| \\hspace{2mm} d(x,y) \\leq \\epsilon \\right \\} $\n",
    "    \n",
    "    \n",
    "\n",
    "- **Directamente denso-alcanzable:** El punto $x$ se dirá denso-alcanzable respecto al punto $y$ si cumple con las siguientes condiciones\n",
    "\n",
    "    1) $x \\in N_{\\epsilon}(y)$\n",
    "    \n",
    "    2) $ | N_{\\epsilon}(y) | \\geq N_{min}$ (*Condición de punto nucleo*)\n",
    "    \n",
    "    \n",
    "    \n",
    "- **Denso-alcanzable:** El punto $x$ se dirá denso-alcanzable respecto al punto $z$ si existe una cadena de puntos $x_1,\\dots, x_n \\subset X$; donde $x_1= x$,  $x_n = z$ y tal que $x_{i+1}$ es directamente denso-alcanzable respecto a $x_i$ para $i=1,\\dots,n-1$\n",
    "\n",
    "Esta última noción viene a extender la defición de directamente denso-alcanzable. La propiedad declara que 2 puntos están conectados si existe una cadena de puntos cada uno de ellos con al menos $N_{min}$ puntos a una distancia menor o igual a $\\epsilon$. Si 2 puntos son denso-alcanzables entre si, diremos que están **denso-conectados**.\n",
    "\n",
    "- **Cluster:** Diremos que un conjunto no vacio $C \\subset X$ es un cluster de $X$ si satisface las siguientes condiciones\n",
    "\n",
    "    1) $\\forall x,y \\in X$, si $x \\in C$ e $y$ es denso-alcanzable desde $x$, entonces $y \\in C$. (maximalidad)\n",
    "    \n",
    "    2) $\\forall x,y \\in X$, entonces $x$ es denso-alcanzable desde $y$. (Conectividad)\n",
    "    \n",
    "    \n",
    "Esta definición nos dice que un Cluster en el sentido de DBSCAN corresponde a un conjunto en el que todos sus elementos se relacionan entre si en el sentido de ser denso-alcanzables. Por otro lado, este conjunto no puede crecer más, ya que contiene todos los puntos alcanzables en principio (maximalidad).\n",
    "\n",
    "En este contexto DBSCAN separa los puntos en $X$ en 3 categorías:\n",
    "\n",
    "- **Puntos núcleos:** Corresponden a los puntos que poseen al menos $N_{min}$ \"$\\epsilon$-vecinos\" ($ | N_{\\epsilon}(y) | \\geq N_{min}$), estos puntos pueden pensarse como ubicados en el interior de un cluster.\n",
    "\n",
    "- **Puntos borde:** Corresponden a los puntos que son alcanzables desde un punto núcleo, pero $ | N_{\\epsilon}(y) | < N_{min}$, corresponden a los puntos en el margen de un cluster.\n",
    "\n",
    "- **Outliers:** Estos son los puntos que no son alcanzable por ningún otro punto, se les denota tambien como *puntos ruido*.\n",
    "\n",
    "Dicho esto, el algoritmo DBSCAN opera recorriendo cada $x \\in X$, determina cuales puntos $y \\in X$ son denso-alcanzables desde el punto actual. Si $x$ es un **punto núcleo** se genera un cluster, por otro lado si $x$ resulta ser un **punto borde** no existirán otros puntos en $X$ que sean alcanzables desde $x$. Luego DBSCAN pasa a revisar el siguiente punto en $X$ hasta completar el recorrido de todos los elementos.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se inicializa el algoritmo DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples = 2, eps = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros `min_samples` indica el número mínimo de puntos núcleo, `eps` es la minima distancia para considerar a dos puntos en el mismo cluster.\n",
    "\n",
    "Se procede a entrenar y obtiener las asignaciones de cada punto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = outlier_detection.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el número total de outliers viene dado por aquellos puntos etiquetados como '-1'. En este caso, se tienen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(clusters).count(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[8, 5])\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2], c=clusters, cmap='viridis', linewidth=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Utilice el clustering anterior para extraer los posible puntos anómalos de la base estudiada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bosque de aislamiento\n",
    "\n",
    "Los métodos basdos en bosques aleatorios permiten trabajar con datos de alta dimensionalidad de manera sencilla. El bosque de aislamiento *Isolation Forest*, consiste en un estimador de puntajes anomalías. Para lograr tal cometido, el algoritmo actua de la siguiente manera:\n",
    "\n",
    "1. Selecciona una variable $x_j$ al azar.\n",
    "2. Selecciona un valor $x^{*}$ al azar entre $(\\min(x_j),\\max(x_j))$.\n",
    "3. Basado en  $x^{*}$, genera una partición del dataset según las filas cuyo valor de la variable $x_j$ es mayor a $x^{*}$ (y respectivamente menor que $x^{*}$). \n",
    "4. Vuelve al paso 1 y aplicando sobre cada dataset inducido por la partición. \n",
    "\n",
    "Este tipo de partición recurrente genera un árbol, donde sus hojas vienen dadas por valores aislados. La cantidad de particiones necesarias para aislar tales puntos viene dada por su distancia con la raiz del árbol. Si tal distancia es pequeña, se espera que el valor aislado sea un outlier, para que esta afirmación tenga sentido estadístico, es necesario generar varios estimadores de aislamiento, dando lugar a un bosque (conjunto de arboles) de aislamiento.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se inicializa el algoritmo y se prueba en los datos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "rfi = IsolationForest(max_samples=1460,\n",
    "                      random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es una clasificación entre puntos normales (1) y anomalías (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas = rfi.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que al proporcionar todo el dataset, entrega un buen aproximado a los outliers introducidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(clas).count(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la visualización se aprecia el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[8, 5])\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2], c=clas, cmap='viridis', linewidth=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Los métodos de sobre elíptico (elliptic envelope),  Local Outlier factor (LOF) y one class support vector machines son bastante eficientes en la detección de outliers. Estudie los objetos:\n",
    "\n",
    "* `neighbors.LocalOutlierFactor`\n",
    "* `covariance.EllipticEnvelope`\n",
    "* `svm.OneClassSVM`\n",
    "\n",
    "\n",
    "\n",
    "1. ¿Como permiten (cada uno) la detección de outliers? \n",
    "2. Discuta las ventajas y desventajas de cada uno.\n",
    "3. Entrene y aplique cada uno de estos métodos.\n",
    "4. Estudie la formaluación matemática de cada uno de estos métodos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
